<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="">
    <meta name="author" content="">
    <link rel="shortcut icon" href="../../assets/ico/favicon.ico">

    <title>Jingjing Wang</title>

    <!-- Bootstrap core CSS -->
    <link href="../dist/css/bootstrap.min.css" rel="stylesheet">

    <!-- Just for debugging purposes. Don't actually copy this line! -->
    <!--[if lt IE 9]><script src="../../assets/js/ie8-responsive-file-warning.js"></script><![endif]-->

    <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
      <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

    <!-- Custom styles for this template -->
    <link href="jumbotron.css" rel="stylesheet">
    <link href="../dist/css/particles.css" rel="stylesheet">
    <style>
        .model-img {
            position: relative;
            min-height: 300px;
            background-color: #f5f5f5;
        }

        .model-img::before {
            content: "Loading...";
            position: absolute;
            top: 50%;
            left: 50%;
            transform: translate(-50%, -50%);
            color: #666;
        }
    </style>
</head>

<body>

    <div class="navbar navbar-inverse navbar-fixed-top" role="navigation">
        <div class="container">
            <div class="navbar-header">
                <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
                    <span class="sr-only">Toggle navigation</span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                </button>
                <!--
          <a class="navbar-brand" href="#">Project name</a>
          -->
            </div>
            <div class="navbar-collapse collapse">
                <ul class="nav nav-pills pull-right">
                    <li class="active"><a href="#">Home</a></li>
                    <li><a href="#Recent-Research">Research</a></li>
                    <li><a href="#publication">Publications</a></li>
                    <!-- <li><a href="#award">Awards</a></li> -->
                    <!-- <li><a href="data.html">Data</a></li> -->
                </ul>
            </div><!--/.navbar-collapse -->
        </div>
    </div>

    <!-- Main jumbotron for a primary marketing message or call to action -->
    <div class="jumbotron">
        <!-- <div id="particles"></div> -->
        <div class="container">
            <div class="row">
                <p></p>

    
                <div class="col-md-2" style="margin-right: 0px;">
                    <img height="160" src="../img/djwang.png" align="left" hspace="6"
                        style="margin-left:-6p;margin-right:10px">
                </div>
                <p></p>
                <div class="col-md-9">
                    <b>
                        <font size="5"> Jingjing Wang ç‹æ™¶æ™¶ </font>
                    </b>
                    <p></p>
                    <p> <b>Associate Professor, Natural Language Processing Lab, Soochow University </b></p>
                    <p> <b> Email: djingwang [at] suda [dot] edu [dot] cn </b></p>
                    <p> <b> Address: No.1, Shizi Street, Suzhou, China, 215006 </b>
                    </p>
                </div>
            </div>

            <div class="row" id="About">
                <div class="col-md-12">
                    <p> I am an Associate Professor at <a target="_blank" href="https://scst.suda.edu.cn/">School of
                            Computer
                            Science and Technology</a>, <a target="_blank" href="http://www.suda.edu.cn/">Soochow
                            University</a>.
                        I am also a Senior Technical Consultant (Part-time) at Microsoft (Asia), China.
                        My research interests focus on Multimodal Computing (especially for Visual-Language
                        Understanding and Generation, Embodied Intelligence), Affective Computing, Large Language
                        Models and AI for
                        Medical Diagnosis.
                        I received my Ph.D. degree from <a target="_blank" href="http://www.suda.edu.cn/">Soochow
                            University</a> in 2019, fortunately advised by <a target="_blank"
                            href="https://scholar.google.com/citations?user=KELQj9cAAAAJ&hl=zh-CN">Prof. Guodong
                            Zhou</a> and <a target="_blank"
                            href="https://scholar.google.com.hk/citations?user=ZRGSxdUAAAAJ&hl=en">Prof. Shoushan
                            Li</a>. During my career, I am also working with <a target="_blank"
                            href="https://scholar.google.com/citations?user=CncXH-YAAAAJ&hl=en">Prof. Min Zhang</a> for
                        advancing Natural Language Processing and Artificial Intelligence
                        technology to benefit humanity.

                    </p>
                    <p>
                        <font color="red">I am actively seeking dedicated students with intellectual curiosity and a
                                strong work ethic to join my research team.</font>
                        Prospective candidates are welcome to submit their academic CV and research interests via email
                        for initial consultation.
                        Regarding recommendation letters, please be advised that I can only provide substantive
                        evaluations for candidates with whom I have maintained at least six months of meaningful
                        academic collaboration.
                        This duration allows me to objectively assess your research competencies, scholarly
                        contributions, and professional development through sustained engagement.
                    </p>
                </div>
            </div>


            <div class="row" id="What's New">
                <div class="col-md-12">
                    <div class="page-header">
                        <h3><b>
                        <font size="4">What's New</font></b></h3>

                        <li style="margin:10px"> ğŸ”¥<b>[May 15, 2025]</b> Our paper "Table-Critic" is
                            accepted by ACL 2025 Main Conference (CCF A). </li>
                        <li style="margin:10px"> ğŸ”¥<b>[Jan. 21, 2025]</b> Two papers ("Omni-SILA" & "Sherlock") are
                            accepted by WWW 2025 (CCF A). </li>

                        <li style="margin:10px"> ğŸ”¥<b>[Dec. 28, 2024]</b> Two students (Yu Tan & Jianing Zhao) in my team
                            achieve the National Scholarship. </li>

                        <li style="margin:10px"> ğŸ”¥<b>[Dec. 10, 2024]</b>
                            "SQLFixAgent: Towards Semantic-Accurate Text-to-SQL Parsing" is accepted by AAAI 2025 (CCF
                            A). </li>

                        <li style="margin:10px"> <b>[Dec. 01, 2024]</b>
                            "TACL: A Trusted Action-enhanced Curriculum Learning Approach</a>" is accepted by
                            Neurocomputing (JCR Q1). </li>

                        <li style="margin:10px"> ğŸ”¥<b>[Jul. 16, 2024]</b> Two papers ("Emotion-enriched Text-to-Motion
                            Generation" & "Hawkeye") are accepted by ACM MM 2024 (CCF A). </li>

                        <li style="margin:10px"> <b>[Feb. 20, 2024]</b> Three papers ("ChatASU", "Weakly-supervised
                            Phrase Grounding" & "TopicDiff") are accepted by COLING 2024 (CCF B). </li>


                    </div>
                </div>
            </div>
        </div>
    </div>

    <div class="container" id="Recent-Research">
        <div class="row" id="Recent-Research">
            <div class="col-md-12">
                <h2><b>
                        <font size="5">Recent Research</font></b></h2>
                <div class="page-header">
                    <h4><b>Foundation Model for Video Understanding</b></h4>
                </div>
                <img width="600" src="../img/hawkeye.svg" align="left" style="margin-left:0px;margin-right:20px"
                    class="model-img">
                <p>
                    The goal is to establish a unified framework for video anomaly detection, advancing precision in
                    identifying and localizing abnormal events across dynamic scenes while enabling interpretable
                    analysis
                    of complex visual patterns.
                </P>
                <p>
                    Starting from real-world applications in surveillance and social media analysis, we introduce
                    Hawkeye (<a target="_blank"
                        href="../works/Hawkeye Discovering and Grounding Implicit Anoma-lous Sentiment in Recon-videos via Scene-enhanced Video Large Language Model.pdf">Zhao
                        et al., ACM MM'24</a>)
                    , the first scene-enhanced video-language model designed for anomaly detection. Hawkeye
                    integrates multimodal context
                    (visual-textual-temporal cues) to recognize subtle anomalies and pinpoint their temporal
                    boundaries in untrimmed videos,
                    This work lays a critical foundation for event typing and spatiotemporal
                    localization in short video understanding.
                </p>
                <p>
                    Building on this, we investigate low-resource scenarios where annotated anomaly data is scarce.
                    Our Continuous
                    Attention Modeling method (<a target="_blank" href="../works/é’ˆå¯¹ä½èµ„æºåœºæ™¯ä¸‹è¿ç»­æƒ…æ„Ÿåˆ†æä»»åŠ¡çš„æŒç»­æ³¨æ„åŠ›å»ºæ¨¡.pdf">Zhang et
                        al.,JOS'23</a>) enhances adaptability by capturing long-range
                    dependencies
                    in sparse anomaly signals. Further, we extend Hawkeye with self-supervised learning to uncover
                    latent patterns
                    across unlabeled videos, improving generalization to unseen anomaly types.
                    To scale solutions, we construct a benchmark suite combining large-scale anomaly annotations and
                    instruction-tuned datasets.
                    This addresses the challenge of diverse event types (e.g., accidents, unusual behaviors) and
                    supports downstream tasks
                    like explainable reasoning (<a target="_blank"
                        href="../works/ChatASU Evoking LLM's Reflexion to Truly Understand Aspect Sentiment in Dialogues.pdf">Liu
                        et al., COLING'2024</a>).
                </p>
                <!-- <p>
                    ç›®æ ‡â€‹â€‹æ˜¯æ„å»ºç»Ÿä¸€çš„è§†é¢‘å¼‚å¸¸æ£€æµ‹æ¡†æ¶ï¼Œåœ¨åŠ¨æ€åœºæ™¯ä¸­æå‡å¼‚å¸¸äº‹ä»¶è¯†åˆ«ä¸å®šä½çš„ç²¾åº¦ï¼ŒåŒæ—¶å®ç°å¯¹å¤æ‚è§†è§‰æ¨¡å¼çš„å¯è§£é‡Šåˆ†æã€‚
                    â€‹ä»å®‰é˜²ç›‘æ§ä¸ç¤¾äº¤åª’ä½“åˆ†æçš„å®é™…éœ€æ±‚å‡ºå‘â€‹â€‹ï¼Œæˆ‘ä»¬æå‡ºé¦–ä¸ªåœºæ™¯å¢å¼ºçš„è§†é¢‘-è¯­è¨€æ¨¡å‹Hawkeyeï¼ˆACM MM-2024ï¼‰ã€‚è¯¥æ¨¡å‹èåˆè§†è§‰ã€
                    æ–‡æœ¬ä¸æ—¶åºçº¿ç´¢ï¼Œèƒ½å¤Ÿè¯†åˆ«çŸ­è§†é¢‘ä¸­ç»†å¾®çš„å¼‚å¸¸è¡Œä¸ºå¹¶ç²¾ç¡®å®šä½å…¶æ—¶ç©ºè¾¹ç•Œï¼ˆå¦‚å›¾2.1æ‰€ç¤ºï¼‰ï¼Œä¸ºçŸ­æ—¶è§†é¢‘çš„äº‹ä»¶ç±»å‹åˆ’åˆ†ä¸æ—¶ç©ºå®šä½å¥ å®šé‡è¦åŸºç¡€ã€‚
                    â€‹â€‹é’ˆå¯¹ä½èµ„æºåœºæ™¯â€‹â€‹ï¼ˆæ ‡æ³¨æ•°æ®ç¨€ç¼ºï¼‰ï¼Œæˆ‘ä»¬æå‡ºè¿ç»­æ³¨æ„åŠ›å»ºæ¨¡æ–¹æ³•ï¼ˆè½¯ä»¶å­¦æŠ¥ï¼Œå¼ ç­‰äººï¼‰ï¼Œé€šè¿‡æ•æ‰ç¨€ç–å¼‚å¸¸ä¿¡å·çš„é•¿ç¨‹ä¾
                    èµ–å…³ç³»æå‡æ¨¡å‹é€‚åº”æ€§ã€‚è¿›ä¸€æ­¥æ‰©å±•Hawkeyeè‡³è‡ªç›‘ç£å­¦ä¹ æ¡†æ¶ï¼ŒæŒ–æ˜æœªæ ‡æ³¨è§†é¢‘çš„æ½œåœ¨æ¨¡å¼ä»¥å¢å¼ºå¯¹æœªè§å¼‚å¸¸ç±»å‹çš„æ³›åŒ–èƒ½åŠ›
                    ï¼ˆèµµç­‰äººï¼Œå®¡ç¨¿ä¸­ï¼‰ã€‚                
                    â€‹â€‹ä¸ºæ‰©å¤§è§£å†³æ–¹æ¡ˆè§„æ¨¡â€‹â€‹ï¼Œæˆ‘ä»¬æ„å»ºäº†èåˆå¤§è§„æ¨¡å¼‚å¸¸æ ‡æ³¨ä¸æŒ‡ä»¤å¾®è°ƒæ•°æ®é›†çš„åŸºå‡†æµ‹è¯•å¥—ä»¶ï¼Œè§£å†³äº‹ä»¶ç±»å‹å¤šæ ·æ€§ï¼ˆå¦‚äº‹æ•…ã€å¼‚å¸¸è¡Œä¸ºï¼‰
                    çš„æŒ‘æˆ˜ï¼Œå¹¶æ”¯æŒå¯è§£é‡Šæ¨ç†ç­‰ä¸‹æ¸¸ä»»åŠ¡ï¼ˆåˆ˜ç­‰äººï¼ŒCOLING-2024ï¼‰ã€‚
                </p> -->
                <p>
                    <b>Papers: </b>
                    (<a target="_blank"
                        href="../works/Hawkeye Discovering and Grounding Implicit Anoma-lous Sentiment in Recon-videos via Scene-enhanced Video Large Language Model.pdf">Zhao
                        et al., ACM MM'24</a>)
                    (<a target="_blank" href="../works/é’ˆå¯¹ä½èµ„æºåœºæ™¯ä¸‹è¿ç»­æƒ…æ„Ÿåˆ†æä»»åŠ¡çš„æŒç»­æ³¨æ„åŠ›å»ºæ¨¡.pdf">Zhang et al.,JOS'23</a>)
                    (<a target="_blank"
                        href="../works/ChatASU Evoking LLM's Reflexion to Truly Understand Aspect Sentiment in Dialogues.pdf">Liu
                        et al., COLING'2024</a>).
                </p>

                <div class="page-header">
                    <h4><b>Foundation Model for Multimodal Generation</b></h4>
                </div>
                <img width="600" src="../img/L3EM.svg" align="left"
                    style="margin-left:0px;margin-right:20px;margin-bottom:10px" class="model-img">
                <p>
                    The goalâ€‹â€‹ is to establish an emotion-driven framework for multimodal large language model (LLM)
                    video generation
                    , achieving cross-modal semantic alignment and high-fidelity emotional action synthesis while
                    enhancing
                    controllability and realism in generated content.
                </p>
                <p>
                    Starting from the core demand of multimodal interaction, we propose LLM-Guided Emotion-Action
                    Synthesis
                    (<a target="_blank"
                        href="../works/Towards Emotion-enriched Text-to-Motion Generation via LLM-guided Limb-level Emotion Manipulating.pdf">Yu
                        et al., ACM MM'2024</a>),
                    the first method to jointly model text emotion semantics and human motion sequences for
                    emotionally rich video generation
                    This work enables precise conveyance of complex emotions (e.g., "excited waving") in synthetic
                    videos, providing critical support for emotion-aware short video editing.
                </p>
                <p>
                    To address cross-modal alignment challenges, we introduce two innovations:
                    1) A Trustworthy Reflection Mechanism
                    (<a target="_blank"
                        href="../works/ChatASU Evoking LLM's Reflexion to Truly Understand Aspect Sentiment in Dialogues.pdf">Liu
                        et al., COLING'24</a>)
                    that dynamically calibrates generation intent with emotional
                    labels, improving semantic coherence in action outputs;
                    2) Latent Diffusion Models
                    (<a target="_blank"
                        href="../works/How to Understand 'Support'? An Implicit-enhanced Causal Inference Approach for Weakly-supervised Phrase Grounding.pdf">Luo
                        et al., COLING'24</a>)
                    that jointly model emotion-themed distributions across text, audio,
                    and motion modalities in latent space, overcoming efficiency bottlenecks of pixel-level
                    generation.

                    To scale practical applications, we construct the first benchmark dataset for emotion-driven
                    action generation,
                    containing 100K+ emotion-annotated human motion sequences and multimodal instruction pairs.
                    Based on this, we propose
                    AutoEmoDirector, a framework enabling users to edit video emotion intensity and motion styles
                    via natural language instructions
                    (e.g., "transform a joyful dance into a sorrowful stroll"), significantly lowering the barrier
                    for video content creation.
                </p>

                <!-- <p>
                    ç›®æ ‡â€‹â€‹æ˜¯æ„å»ºæƒ…æ„Ÿé©±åŠ¨çš„å¤§æ¨¡å‹è§†é¢‘ç”Ÿæˆæ¡†æ¶ï¼Œå®ç°è·¨æ¨¡æ€è¯­ä¹‰å¯¹é½ä¸é«˜ä¿çœŸæƒ…æ„ŸåŠ¨ä½œåˆæˆï¼ŒåŒæ—¶æå‡ç”Ÿæˆç»“æœçš„å¯æ§æ€§ä¸çœŸå®æ€§ã€‚
                    â€‹â€‹ä»å¤šæ¨¡æ€äº¤äº’çš„æ ¸å¿ƒéœ€æ±‚å‡ºå‘â€‹â€‹ï¼Œæˆ‘ä»¬æå‡ºLLM-Guided Emotion-Action Synthesisï¼ˆACM MM-2024ï¼‰ï¼Œé€šè¿‡å¤§è¯­è¨€æ¨¡å‹å¼•å¯¼è‚¢ä½“
                    åŠ¨ä½œçš„ç»†ç²’åº¦æƒ…æ„Ÿè¡¨è¾¾ï¼ˆå¦‚å›¾2.2æ‰€ç¤ºï¼‰ã€‚è¯¥æ–¹æ³•é¦–æ¬¡å°†æ–‡æœ¬æƒ…æ„Ÿè¯­ä¹‰ä¸äººä½“è¿åŠ¨åºåˆ—è”åˆå»ºæ¨¡ï¼Œç”Ÿæˆçš„è§†é¢‘èƒ½ç²¾å‡†ä¼ é€’æ„¤æ€’ã€å–œ
                    æ‚¦ç­‰å¤æ‚æƒ…æ„Ÿï¼ˆå¦‚"æ¿€åŠ¨åœ°æŒ¥æ‰‹"ï¼‰ï¼Œä¸ºæƒ…æ„ŸåŒ–çŸ­è§†é¢‘ç¼–è¾‘æä¾›å…³é”®æŠ€æœ¯æ”¯æŒã€‚  
                    â€‹â€‹é’ˆå¯¹è·¨æ¨¡æ€å¯¹é½éš¾é¢˜â€‹â€‹ï¼Œæˆ‘ä»¬æå‡ºä¸¤ç§åˆ›æ–°æ–¹æ³•ï¼š1ï¼‰æ”¹è¿›å¤§è¯­è¨€æ¨¡å‹çš„å¯ä¿¡åæ€æœºåˆ¶ï¼ˆLiu et al., COLING-2024ï¼‰ï¼Œé€šè¿‡åŠ¨æ€æ ¡
                    å‡†ç”Ÿæˆæ„å›¾ä¸æƒ…æ„Ÿæ ‡ç­¾çš„ä¸€è‡´æ€§ï¼Œæå‡åŠ¨ä½œç”Ÿæˆçš„è¯­ä¹‰åˆç†æ€§ï¼›2ï¼‰è®¾è®¡éšç©ºé—´æ‰©æ•£æ¨¡å‹ï¼ˆLuo et al., COLING-2024ï¼‰ï¼Œåœ¨æ½œåœ¨
                    ç©ºé—´è”åˆå»ºæ¨¡æ–‡æœ¬ã€éŸ³é¢‘ä¸åŠ¨ä½œæ¨¡æ€çš„æƒ…æ„Ÿä¸»é¢˜åˆ†å¸ƒï¼Œçªç ´ä¼ ç»Ÿåƒç´ çº§ç”Ÿæˆçš„æ•ˆç‡ç“¶é¢ˆã€‚
                    â€‹â€‹ä¸ºæ¨åŠ¨è§„æ¨¡åŒ–åº”ç”¨â€‹â€‹ï¼Œæˆ‘ä»¬æ„å»ºäº†é¦–ä¸ªæƒ…æ„ŸåŠ¨ä½œç”ŸæˆåŸºå‡†æ•°æ®é›†ï¼ŒåŒ…å«10ä¸‡+æ¡å¸¦æƒ…æ„Ÿæ ‡æ³¨çš„äººä½“åŠ¨ä½œåºåˆ—ä¸å¤šæ¨¡æ€æŒ‡ä»¤å¯¹ã€‚åŸºäºæ­¤ï¼Œ
                    æˆ‘ä»¬æå‡ºAutoEmoDirectoræ¡†æ¶ï¼Œæ”¯æŒç”¨æˆ·é€šè¿‡è‡ªç„¶è¯­è¨€æŒ‡ä»¤å®æ—¶ç¼–è¾‘è§†é¢‘ä¸­çš„æƒ…æ„Ÿå¼ºåº¦ä¸åŠ¨ä½œé£æ ¼ï¼ˆå¦‚"å°†æ¬¢å¿«èˆè¹ˆè½¬ä¸ºæ‚²ä¼¤æ¼«æ­¥
                    "ï¼‰ï¼Œæ˜¾è‘—é™ä½è§†é¢‘åˆ›ä½œé—¨æ§›ã€‚
                </p> -->

                <p>
                    <b>Papers: </b>
                    (<a target="_blank"
                        href="../works/Towards Emotion-enriched Text-to-Motion Generation via LLM-guided Limb-level Emotion Manipulating.pdf">Yu
                        et al., ACM MM'2024</a>)
                    (<a target="_blank"
                        href="../works/How to Understand 'Support'? An Implicit-enhanced Causal Inference Approach for Weakly-supervised Phrase Grounding.pdf">Luo
                        et al., COLING'24</a>)
                    (<a target="_blank"
                        href="../works/ChatASU Evoking LLM's Reflexion to Truly Understand Aspect Sentiment in Dialogues.pdf">Liu
                        et al., COLING'24</a>)
                </p>
            </div>


        </div>
    </div>
    <div class="container" id="publication">
        <div class="row" id="publication">
            <div class="col-md-12">
                <h2><b>
                        <font size="5">Publications</font></b></h2>
                <div class="sectionbody">
                    <div class="paragraph">
                        <p><b>2025</b></p>
                    </div>
                    <div class="olist arabic">
                        <ol class="arabic">
                          <li>
                                <p>
                                    Peiying Yu, Guoxing Chen, <strong>Jingjing Wang</strong>.
                                    <em>Table-Critic: A Multi-Agent Framework for Collaborative Criticism and Refinement in Table Reasoning</em>. Proceedings of Annual Meeting of the Association for Computational Linguistics
                                     (<strong>ACL, Main Conference</strong>), 2025. (CCF A, Corresponding Author)
                                </p>
                            </li>

                            
                            <li>
                                <p>
                                    Junxiao Ma, <strong>Jingjing Wang</strong>, Jiamin Luo, Peiying Yu, Guodong Zhou.
                                    <em>Sherlock:
                                        Towards Multi-scene Video Abnormal Event Extraction and Localization via a
                                        Global-local Spatial-sensitive LLM</em>. The Web Conference
                                    (<strong>WWW</strong>), 2025. (CCF A, Corresponding Author)
                                </p>
                            </li>
                            <li>
                                <p>
                                    Jiamin Luo, <strong>Jingjing Wang</strong>, Junxiao Ma, Yujie Jin, Shoushan Li,
                                    Guodong Zhou.
                                    <em>Omni-SILA: Towards Omni-scene Driven Visual Sentiment Identifying, Locating
                                        and Attributing in Videos</em>. The Web Conference (<strong>WWW</strong>),
                                    2025. (CCF A, Corresponding Author)
                                </p>
                            </li>
                            <li>
                                <p>
                                    Jipeng Cen, Jiaxin Liu, Zhixu Li, <strong>Jingjing Wang</strong>. <em>SQLFixAgent:
                                        Towards
                                        Semantic-Accurate Text-to-SQL Parsing via Consistency-Enhanced Multi-Agent
                                        Collaboration</em>. The AAAI Conference on Artificial Intelligence
                                    (<strong>AAAI</strong>), 2025. (CCF A, Corresponding Author)
                                </p>
                            </li>
                            <li>
                                <p>
                                    Luo J., <strong>Wang J.</strong>, Zhou G. <em>Multi-modal Reliability-aware
                                        Affective Computing</em>. Ruan Jian Xue Bao/Journal of Software
                                    (<strong>JOS</strong>), 2025, 36(2):537-553. (CCF A in Chinese Journal, Corresponding Author)
                                </p>
                            </li>
                            <li>
                                <p>
                                    Tan Yu, <strong>Jingjing Wang</strong>, Jiamin Luo, Jiawen Wang, Guodong Zhou.
                                    <em>TACL: A
                                        Trusted Action-enhanced Curriculum Learning Approach to Multimodal Affective
                                        Computing</em>. Neurocomputing, 2025, 620:129195. (SCI, JCR Q1, Corresponding Author)
                                </p>
                            </li>
                        </ol>
                    </div>

                    <div class="paragraph">
                        <p><b>2024</b></p>
                    </div>
                    <div class="olist arabic">
                        <ol class="arabic">
                            <li>
                                <p>
                                    Jianing Zhao, <strong>Jingjing Wang</strong>, Yujie Jin, Jiamin Luo, Guodong Zhou. <em>Hawkeye:
                                        Discovering and
                                        Grounding Implicit Anomalous Sentiment in Recon-videos via Scene-enhanced
                                        Video Large Language Model</em>. Proceedings of ACM International Conference
                                    on Multimedia (<strong>MM</strong>), 2024, 592-601. (CCF A, Corresponding Author)
                                </p>
                            </li>
                            <li>
                                <p>
                                    Tan Yu, <strong>Jingjing Wang</strong>, Jiawen Wang, Jiamin Luo, Guodong Zhou. <em>Towards Emotion-enriched
                                        Text-to-Motion Generation via LLM-guided Limb-level Emotion
                                        Manipulating</em>. Proceedings of ACM International Conference on Multimedia
                                    (<strong>MM</strong>), 2024, 612-621. (CCF A, Corresponding Author)
                                </p>
                            </li>
                            <li>
                                <p>
                                    Han Zhang, <strong>Jingjing Wang</strong>, Jiamin Luo, Guodong Zhou. <em>Continual Attention
                                        Modeling for Sucessive Sentiment Analysis in Low resource Scenarios</em>.
                                    Ruan Jian Xue Bao/Journal of Software (<strong>JOS</strong>), 2024,
                                    35(12):5470-5486. (CCF A in Chinese
                                    Journal, Corresponding Author)
                                </p>
                            </li>
                            <li>
                                <p>
                                    Yiding Liu, <strong>Jingjing Wang</strong>, Jiamin Luo, Tao Zeng, Guodong Zhou. <em>ChatASU: Evoking LLM's
                                        Reflexion to Truly Understand Aspect Sentiment in Dialogues</em>.
                                    Proceedings of International Conference on Computational Linguistics
                                    (<strong>COLING</strong>), 2024, 3075-3085. (CCF B, Corresponding Author)
                                </p>
                            </li>
                            <li>
                                <p>
                                    Jiamin Luo, Jianing Zhao, <strong>Jingjing Wang</strong>, Guodong Zhou. <em>How to Understand
                                        'Support'? An Implicit-enhanced Causal Inference Approach for
                                        Weakly-supervised Phrase Grounding</em>. International Conference on
                                    Computational Linguistics (<strong>COLING</strong>), 2024. (CCF B, Corresponding Author)
                                </p>
                            </li>
                            <li>
                                <p>
                                    Jiamin Luo, <strong>Jingjing Wang</strong>, Guodong Zhou. <em>TopicDiff: A Topic-enriched
                                        Diffusion Approach for Multimodal Conversational Emotion Detection</em>.
                                    International Conference on Computational Linguistics (<strong>COLING</strong>),
                                    2024. (CCF B, Corresponding Author)
                                </p>
                            </li>
                            <li>
                                <p>
                                    Jiamin Luo, <strong>Jingjing Wang</strong>, Guodong Zhou. <em>Topic-Enriched Variational
                                        Transformer for Conversational Emotion Detection</em>. CCF International
                                    Conference on Natural Language Processing and Chinese Computing
                                    (<strong>NLPCC</strong>),
                                    2024, 3-15. (CCF C, Corresponding Author)
                                </p>
                            </li>
                        </ol>
                    </div>

                    <div class="paragraph">
                        <p><b>2023</b></p>
                    </div>
                    <div class="olist arabic">
                        <ol class="arabic">
                            <li>
                                <p>
                                    Yiding Liu, <strong>Jingjing Wang</strong>, Jiamin Luo, Guodong Zhou. <em>LLM-Grounded Conversation
                                        Aspect Sentiment Understanding via Muti-Agent Consistency Reflection</em>.
                                    Journal of Software (<strong>JOS</strong>), 2021. (CCF A in Chinese Journal, Corresponding Author)
                                </p>
                            </li>
                            <li>
                                <p>
                                    Jianing Zhao, <strong>Jingjing Wang</strong>, Jiamin Luo, Guodong Zhou. <em>Implicit-enhanced Causal
                                        Modeling for Phrasal Visual Grounding</em>. Ruan Jian Xue Bao/Journal of
                                    Software (<strong>JOS</strong>), 2021. (CCF A in Chinese Journal, Corresponding Author)
                                </p>
                            </li>
                            <li>
                                <p>
                                    <strong>Jingjing Wang</strong>, Jiamin Luo, Guodong Zhou. <em>Fine-Grained Question-Answer
                                        Matching via Sentence-Aware Contrastive Self-supervised Transfer</em>. CCF
                                    International Conference on Natural Language Processing and Chinese Computing
                                    (<strong>NLPCC</strong>), 2023, 616-628. (CCF C)
                                </p>
                            </li>
                        </ol>
                    </div>

                    <div class="paragraph">
                        <p><b>2022</b></p>
                    </div>
                    <div class="olist arabic">
                        <ol class="arabic">
                            <li>
                                <p>
                                    Xiaoya Gao, <strong>Jingjing Wang</strong>, Shoushan Li, Min Zhang, Guodong Zhou.
                                    <em>Cognition-driven multimodal
                                        personality classification</em>. Science China Information Sciences
                                    (<strong>SCIS</strong>), 2022, 65(10). (CCF A, SCI Q1, Corresponding Author)
                                </p>
                            </li>
                        </ol>
                    </div>

                    <div class="paragraph">
                        <p><b>2021</b></p>
                    </div>
                    <div class="olist arabic">
                        <ol class="arabic">
                            <li>
                                <p>
                                    Xiaoya Gao, <strong>Jingjing Wang</strong>, Shoushan Li, Guodong Zhou. <em>Cognition-Driven
                                        Real-Time Personality Detection via Language-Guided Contrastive Visual
                                        Attention</em>. IEEE International Conference on Multimedia and Expo
                                    (<strong>ICME</strong>), 2021, 1-6. (CCF B, Corresponding Author)
                                </p>
                            </li>

                        </ol>
                    </div>

                    <div class="paragraph">
                        <p><b>2020</b></p>
                    </div>
                    <div class="olist arabic">
                        <ol class="arabic">
                            <li>
                                <p>
                                    Xiao Chen, Changlong Sun, <strong>Jingjing Wang</strong>, Shoushan Li, Luo Si, Min
                                    Zhang, Guodong Zhou. <em>Aspect Sentiment
                                        Classification with Document-level Sentiment Preference Modeling</em>.
                                    Proceedings of Annual Meeting of the Association for Computational Linguistics
                                    (<strong>ACL</strong>), 2020, 3667-3677. (CCF A, Corresponding Author)
                                </p>
                            </li>
                            <li>
                                <p>
                                    <strong>Jingjing Wang</strong>, Jiancheng Wang, Changlong Sun, Shoushan Li,
                                    Xiaozhong Liu, Luo Si, Min Zhang, Guodong Zhou. <em>Sentiment
                                        classification in customer service dialogue with
                                        topic-aware multi-task learning</em>. Proceedings of the AAAI Conference on
                                    Artificial Intelligence (<strong>AAAI</strong>), 2020, 9177-9184. (CCF A, Corresponding Author)
                                </p>
                            </li>
                            <li>
                                <p>
                                    Minghui An, <strong>Jingjing Wang</strong>, Shoushan Li, Guodong Zhou.
                                    <em>Multimodal
                                        topic-enriched auxiliary learning for depression detection</em>. Proceedings
                                    of the International Conference on Computational Linguistics
                                    (<strong>COLING</strong>), 2020, 1078-1089. (CCF B, Corresponding Author)
                                </p>
                            </li>
                        </ol>
                    </div>
                </div>
            </div>
        </div>
    </div>

    <div class="container">
        <div class="row" id="publication-before-2019">
            <div class="col-md-12">
                <div class="sectionbody">
                    <div class="paragraph">
                        <p><b>2019</b></p>
                    </div>
                    <div class="olist arabic">
                        <ol class="arabic">
                            <li>
                                <p>
                                    <strong>Jingjing Wang</strong>, Changlong Sun, Shoushan Li, Jiancheng Wang, Luo Si,
                                    Min Zhang,
                                    Xiaozhong Liu, Guodong Zhou. <em>Human-Like Decision Making:
                                        Document-level Aspect Sentiment Classification via Hierarchical
                                        Reinforcement Learning</em>. Proceedings of the Conference on Empirical
                                    Methods in Natural Language Processing (<strong>EMNLP</strong>), 2019,
                                    5585-5594. (CCF B)
                                </p>
                            </li>
                            <li>
                                <p>
                                    <strong>Jingjing Wang</strong>, Changlong Sun, Shoushan Li, Xiaozhong Liu, Min
                                    Zhang,
                                    Luo Si, Guodong Zhou. <em>Aspect Sentiment Classification Towards
                                        Question-Answering with Reinforced Bidirectional Attention Network</em>.
                                    Proceedings of Annual Meeting of the Association for Computational Linguistics
                                    (<strong>ACL</strong>), 2019. (CCF A)
                                </p>
                            </li>
                            <li>
                                <p>
                                    Hanqian Wu, Shangbin Zhang, <strong>Jingjing Wang</strong>, Mumu Liu, Shoushan Li.
                                    <em>Multi-label Aspect
                                        Classification on Question-Answering Text with Contextualized Attention-Based
                                        Neural Network</em>. Proceedings of China Conference on Chinese Language
                                    Processing
                                    (<strong>CCL</strong>), 2019, 479-491. (EI, Corresponding Author)
                                </p>
                            </li>
                        </ol>
                    </div>

                    <div class="paragraph">
                        <p><b>2018</b></p>
                    </div>
                    <div class="olist arabic">
                        <ol class="arabic">
                            <li>
                                <p>
                                    <strong>Jingjing Wang</strong>, Jie Li, Shoushan Li, Yangyang Kang, Min Zhang,
                                    Luo Si, Guodong Zhou. <em>Aspect Sentiment Classification with both Word-level
                                        and Clause-level Attention Networks</em>. Proceedings of International Joint
                                    Conference on Artificial Intelligence (<strong>IJCAI</strong>), 2018, 4439-4445.
                                    (CCF A)
                                </p>
                            </li>
                            <li>
                                <p>
                                    <strong>Jingjing Wang</strong>, Shoushan Li, Mingqi Jiang, Hanqian Wu, Guodong
                                    Zhou. <em>Cross-media User Profiling with Joint Textual and Social User
                                        Embedding</em>. Proceedings of International Conference on Computational
                                    Linguistics (<strong>COLING</strong>), 2018, 246-251. (CCF B)
                                </p>
                            </li>
                            <li>
                                <p>
                                    Chenlin Shen, Changlong Sun, <strong>Jingjing Wang</strong>, Yangyang Kang,
                                    Shoushan Li, Xiaozhong Liu, Luo Si, Min Zhang, Guodong Zhou. <em>Sentiment
                                        Classification towards Question-Answering with Hierarchical Matching
                                        Network</em>. Proceedings of Conference on Empirical Methods in Natural
                                    Language Processing (<strong>EMNLP</strong>), 2018, 3654-3663. (CCF B)
                                </p>
                            </li>
                            <li>
                                <p>
                                    Huan Liu, <strong>Jingjing Wang</strong>, Shoushan Li, Guodong Zhou.
                                    <em>Semi-supervised Sentiment Classification Based on Auxiliary Task Learning</em>.
                                    In Proceeding of NLPCC-2018. (CCF C)
                                </p>
                            </li>
                            <li>
                                <p>
                                    Hanqian Wu, Mumu Liu, <strong>Jingjing Wang</strong>, Jue Xie, Chenlin Shen.
                                    <em>Question-Answering Aspect Classification with Hierarchical Attention
                                        Network</em>. In Proceeding of CCL-2018, pp. 225-237. (EI, Corresponding Author)
                                </p>
                            </li>
                            <li>
                                <p>
                                    Hanqian Wu, Mumu Liu, <strong>Jingjing Wang</strong>, Jue Xie, Shoushan Li.
                                    <em>Question-Answering Aspect Classification with Multi-attention
                                        Representation</em>. In Proceeding of CCIR-2018, pp.78-89. (EI)
                                </p>
                            </li>
                        </ol>
                    </div>

                    <div class="paragraph">
                        <p><b>2017</b></p>
                    </div>
                    <div class="olist arabic">
                        <ol class="arabic">
                            <li>
                                <p>
                                    <strong>Jingjing Wang</strong>, Shoushan Li, Guodong Zhou. <em>Joint Learning on
                                        Relevant User Attributes in Micro-blog</em>. Proceedings of International
                                    Joint Conference on Artificial Intelligence (<strong>IJCAI</strong>), 2017,
                                    4130-4136. (CCF A)
                                </p>
                            </li>
                            <li>
                                <p>
                                    Dong Zhang, Shoushan Li, <strong>Jingjing Wang</strong>. <em>Semi-supervised
                                        Question Classification with Jointly Learning Question and Answer
                                        Representations</em>. Journal of Chinese Information Processing, vol. 31(1),
                                    2017. (Chinese Core Journal)
                                </p>
                            </li>
                            <li>
                                <p>
                                    Jing Chen, Shoushan Li, <strong>Jingjing Wang</strong>, Guodong Zhou. <em>User age
                                        prediction by combining classification and regression</em>. Sci Sin Inform,
                                    2017, 47: 1095â€“1108, doi: 10.1360/N112016-00278. (Chinese Core Journal)
                                </p>
                            </li>
                        </ol>
                    </div>

                    <div class="paragraph">
                        <p><b>2015</b></p>
                    </div>
                    <div class="olist arabic">
                        <ol class="arabic">
                            <li>
                                <p>
                                    <strong>Jingjing Wang</strong>, Yunxia Xue, Shoushan Li, Guodong Zhou.
                                    <em>Leveraging Interactive Knowledge and Unlabeled Data in Gender Classification
                                        with Co-training</em>. Proceedings of International Conference on Database
                                    Systems for Advanced Applications (<strong>DASFAA</strong>), 2015, 246-251. (CCF
                                    B)
                                </p>
                            </li>
                            <li>
                                <p>
                                    Shoushan Li, <strong>Jingjing Wang</strong>, Guodong Zhou, Hanxiao Shi.
                                    <em>Interactive Gender Inference with Integer Linear Programming</em>.
                                    Proceedings of International Joint Conference on Artificial Intelligence
                                    (<strong>IJCAI</strong>), 2015, 2341-2347. (CCF A)
                                </p>
                            </li>
                            <li>
                                <p>
                                    Shoushan Li, Lei Huang, <strong>Jingjing Wang</strong>, Guodong Zhou.
                                    <em>Semi-Stacking for Semi-supervised Sentiment Classification</em>. Proceedings
                                    of Annual Meeting of the Association for Computational Linguistics
                                    (<strong>ACL</strong>), 2015, 27-31. (CCF A)
                                </p>
                            </li>
                            <li>
                                <p>
                                    Lei Huang, Shoushan Li, <strong>Jingjing Wang</strong>. <em>User-Type Classification
                                        in Micro-Blog Based on Information of Authenticated User</em>. Journal of
                                    Frontiers of Computer Science and Technology, vol. 9(6), 2015. (Chinese Core
                                    Journal)
                                </p>
                            </li>
                            <li>
                                <p>
                                    <strong>Jingjing Wang</strong>, Shoushan Li, Lei Huang. <em>User Gender
                                        Classification in Chinese Microblog</em>. Journal of Chinese Information
                                    Processing, vol. 28(6), 2014. (Chinese Core Journal)
                                </p>
                            </li>
                            <li>
                                <p>
                                    Zhu zhu, <strong>Jingjing Wang</strong>, Shoushan Li, Guodong Zhou.
                                    <em>Interactive Gender Inference in Social Media</em>. Proceedings of
                                    International Conference on Database Systems for Advanced Applications
                                    (<strong>DASFAA</strong>), 2015, 252-258. (CCF B)
                                </p>
                            </li>

                        </ol>
                    </div>

                </div>
            </div>
        </div>
    </div>


    <div class="container" id="award">
        <div class="row" id="award">
            <div class="col-md-12">
                <h2><b>
                        <font size="5">Awards & Honors</font></b></h2>
                <ul>

                    <li>
                        <p>Outstanding Expert and Supervisor of Microsoft (2022)</p>
                    </li>
                    <li>
                        <p>Outstanding Graduate PhD Student of Soochow University (2019)</p>
                    </li>
                    <li>
                        <p>Suzhou Industrial Park Scholarship (2018)</p>
                    </li>
                    <li>
                        <p>National Scholarship for Ph.D. (2017)</p>
                    </li>
                    <li>
                        <p>Ph.D. Scholarship of Soochow University (2017)</p>
                    </li>
                    <li>
                        <p>National Scholarship for Master (2016)</p>
                    </li>
                    <li>
                        <p>Outstanding Graduate Student of Soochow University (2016)</p>
                    </li>
                    <li>
                        <p>Suzhou Industrial Park Scholarship (2015) </p>
                    </li>
                    <li>
                        <p>etc. </p>
                    </li>
                </ul>
            </div>
        </div>
    </div>

    <div class="container" id="academic-services">
        <div class="row" id="academic-services">
            <div class="col-md-12">
                <h3><b>
                        <font size="5">Academic Services</font></b></h3>
                <ul>
                    <li>
                        <p><b>Technical Program Committee (Area Chair & PC)</b></p>
                    </li>
                    <li>
                        <p>ACL: Annual Meeting of the Association for Computational Linguistics, Area Chair</p>
                    </li>
                    <li>
                        <p>EMNLP: Conference on Empirical Methods in Natural Language Processing, Area Chair</p>
                    </li>
                    <li>
                        <p>AAAI: Association for the Advancement of Artificial Intelligence, PC</p>
                    </li>
                    <li>
                        <p>IJCAI: International Joint Conference on Artificial Intelligence, PC</p>
                    </li>
                    <li>
                        <p>etc.</p>
                    </li>

                    <li>
                        <p><b>Journal Reviewer</b></p>
                    </li>
                    <li>
                        <p>TASLP: IEEE/ACM Transactions on Audio, Speech, and Language Processing</p>
                    </li>
                    <li>
                        <p>TALLIP: ACM Transactions on Asian and Low-Resource Language Information Processing</p>
                    </li>
                    <li>
                        <p>SCIS: Science China Information Sciences</p>
                    </li>
                    <li>
                        <p>Science China</p>
                    </li>
                    <li>
                        <p>Acta Automatica Sinica</p>
                    </li>
                    <li>
                        <p>Journal of Chinese Information Processing</p>
                    </li>
                    <li>
                        <p>etc.</p>
                    </li>

                    <li>
                        <p><b>Academic Presentations and Exchanges</b></p>
                    </li>
                    <li>
                        <p>2016-2021: Academic reports and exchanges at top conferences including ACL, AAAI, IJCAI</p>
                    </li>
                    <li>
                        <p>2019: Academic report and exchange at Zhejiang Tailong Commercial Bank, Suzhou Industrial
                            Park Headquarters</p>
                    </li>
                    <li>
                        <p>2019: Invited talk at Ecovacs, Suzhou</p>
                    </li>
                    <li>
                        <p>2022: Academic report and exchange at Alibaba Ant Financial</p>
                    </li>
                    <li>
                        <p>2023: Academic report and exchange at the establishment of NLPAI-SCHOOL, Microsoft Asia
                            Engineering Institute, Suzhou</p>
                    </li>
                    <li>
                        <p>etc.</p>
                    </li>
                </ul>
            </div>
        </div>
    </div>

    <div class="container" id="research-grants">
        <div class="row" id="research-grants">
            <div class="col-md-12">
                <h2><b>
                        <font size="5">Research Grants</font></b></h2>
                <ul>
                    <div class="sectionbody">
                        <div class="paragraph">
                            <p>As Principle Investigator</p>
                        </div>
                        <div class="olist arabic">
                            <ol class="arabic">
                                <li>
                                    <p>
                                        Key Technology Research on Attribute-level Sentiment Analysis for Conversational
                                        Texts (No. 62006166: 240K RMB: 2021.01â€“2023.12)<br>
                                        Supported by the National Natural Science Foundation of China (NSFC Young
                                        Scientist
                                        Fund Project)
                                    </p>
                                </li>
                                <li>
                                    <p>
                                        Research on Chinese Single-document Automatic Summarization Based on Discourse
                                        Structure Analysis (No. 61976146: 560K RMB: 2020.01â€“2023.12)<br>
                                        Supported by the National Natural Science Foundation of China (NSFC General
                                        Program)
                                    </p>
                                </li>
                                <li>
                                    <p>
                                        Resource Construction and Key Technology Research on Sentiment Information
                                        Extraction from Question-answer Texts (No. 2019M661930: 80K RMB:
                                        2020.01â€“2022.12)<br>
                                        Supported by China Postdoctoral Science Foundation (CPSF)
                                    </p>
                                </li>
                            </ol>
                        </div>

                        <div class="paragraph">
                            <p><b>As Co-investigator</b></p>
                        </div>
                        <div class="olist arabic">
                            <ol class="arabic">
                                <li>
                                    <p>
                                        Scene-based Knowledge Graph for Language Understanding and Generation
                                        (Sub-project
                                        No. 2020AAA0108604: 6,650K RMB: 2020.11â€“2023.10)<br>
                                        Supported by the National Key Research and Development Program
                                    </p>
                                </li>
                            </ol>
                        </div>
                    </div>
                </ul>
            </div>
        </div>
    </div>
    <div class="container" id="contact">
        <!-- Example row of columns -->

        <hr>

        <footer>
            <!-- <p>&copy; Jingjing Wang 2023</p> -->
        </footer>
    </div> <!-- /container -->


    <!-- Bootstrap core JavaScript
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.0/jquery.min.js"></script>
    <script src="../dist/js/bootstrap.min.js"></script>
    <script src="../dist/js/particles.min.js"></script>
    <script>
        document.addEventListener('DOMContentLoaded', function () {
            particlesJS('particles', {
                "particles": {
                    "number": {
                        "value": 80,
                        "density": {
                            "enable": true,
                            "value_area": 800
                        }
                    },
                    "color": {
                        "value": "#cccccc"
                    },
                    "shape": {
                        "type": "circle",
                        "stroke": {
                            "width": 0,
                            "color": "#000000"
                        }
                    },
                    "opacity": {
                        "value": 0.5,
                        "random": false
                    },
                    "size": {
                        "value": 3,
                        "random": true
                    },
                    "line_linked": {
                        "enable": true,
                        "distance": 150,
                        "color": "#cccccc",
                        "opacity": 0.4,
                        "width": 1
                    },
                    "move": {
                        "enable": true,
                        "speed": 2,
                        "direction": "none",
                        "random": false,
                        "straight": false,
                        "out_mode": "out",
                        "bounce": false
                    }
                },
                "interactivity": {
                    "detect_on": "canvas",
                    "events": {
                        "onhover": {
                            "enable": true,
                            "mode": "grab"
                        },
                        "onclick": {
                            "enable": true,
                            "mode": "push"
                        },
                        "resize": true
                    },
                    "modes": {
                        "grab": {
                            "distance": 140,
                            "line_linked": {
                                "opacity": 1
                            }
                        },
                        "push": {
                            "particles_nb": 4
                        }
                    }
                },
                "retina_detect": true
            });
        });
    </script>
</body>

</html>
